\section{SLAM}

\gls{SLAM}\cite{FirstSLAMMention}\cite{SLAMIntro} refers to both the problem and the algorithms that tries to solve it. The problem is to incrementally build a map of landmark observations, while also locating the agent which observes these measurements in the map. Popular solutions to this include EKF-SLAM\cite{EKFSLAM}, FastSLAM\cite{FastSLAM1} and factor graph\cite{Dellaert} solutions like iSAM2\cite{iSAM2}.

The problem is usually formulated probabilistic. The important variables used in the formulation are the following:

\begin{itemize}
    \item $x_k$: The pose of the agent at time $k$. 
    \item $u_k$: The vector of inputs that move the agent to the pose $x_k$ at time $k$.
    \item $m_i$: A vector containing the location of the $i$th landmark in the map frame. 
    \item $z_k$: A vector of all the measurements of the relative positions of landmarks that can be measured from the agent at time $k$.
\end{itemize}

There is also a number of sets that are needed

\begin{itemize}
    \item $X_{0:k} = \{x_0,x_1,...x_k\} = \{X_{0:k-1},x_k\}$: The history of all the agents poses.
    \item $U_{1:k} = \{u_1,u_2,...,u_k\} = \{U_{1:k-1},u_k\}$: The history of all control inputs.
    \item $m=\{m_1,m_2,...,m_n\}$: The set of all landmarks, also called the map.
    \item $Z_{0:k}=\{z_0,z_2,...,z_k\} = \{Z_{0:k-1},z_k\}$: The set of all landmark observations that have been measured until time $k$.
\end{itemize}

The problem can now be formulated as computing the probability distribution 

\begin{equation}
    P(x_k,m|Z_{0:k},U_{0:k},x_0)
\end{equation}

at every time step $k$. This is the joint posterior density of the location of all landmarks in the map and the agents pose, given all the measurements that have been seen so far and the control inputs that have been applied to the agent, as well as the initial pose. 

One usually wants to compute this distribution recursively. This is done by using the previously computed distribution $P(x_{k-1},m|Z_{0:k-1},U_{0,k-1},x_0)$ at the previous time step $k-1$, together with a model for the next pose of the agent given the current input and pose, 

\begin{equation}
    P(x_k|x_{k-1},u_k)
\end{equation}

and a measurement model, giving the probability of making an observation $z_k$ of landmarks, when both the agents pose and the landmark locations are known

\begin{equation}
    P(z_k|x_k,m)
\end{equation}

The standard way of doing it now is doing a two part update to get the desired distribution. First a time update, that uses the motion model to find an estimate for the agents location


\begin{align}
    P(x_k,m|Z_{0:k},U_{0:k}, x_0) 
     = & \int P(x_k|x_{k-1},u_k) \\
    & \times P(k_{k-1},m|Z_{0:k-1},U_{0:k-1},x_0)dx_{k-1}
\end{align}

The second update is to include the effect of measurements, 

\begin{equation}
\begin{split}
    P(x_k,m|Z_{0:k},U_{0:k},x_0) 
    = \frac{P(z_k|x_k,m)P(x_k,m|Z_{0:k-1},U_{0:k},x_0)}{P(z_k|Z_{0:k-1},U_{0:k})}
\end{split}
\end{equation}

How to describe the motion model and measurement model, and how to do the two updates differ from method to method, but this is the main idea that lies behind them all. The SLAM problem has been said to have been solved, but this is only meant theoretically; in practice there are still problems with all of the methods. The main challenges that are still not overcome completely are 

\begin{itemize}
    \item How to update in real time.
    \item How to build a consistent map that does not make the computational demand grow exponentially.
    \item How to associate measurements to landmarks.
\end{itemize}

\subsection{iSAM2}

A state of the art solution to the above problems is \gls{iSAM2}\cite{iSAM2}. It adds information incrementally without the need for periodic batch operations. This means the computational demand does not grow unbounded over time. 

This is achieved because of the following

\begin{itemize}
    \item A problem representation that makes it possible to change only affected variables when new information arrives.
    \item A fluid relinearisation scheme that only relinearises variables when they deviate from the linearisation point by a certain amount.
    \item A smart way of reordering the new structure after addition of new variables that ensures as few as possible of the variables will be affected by the next set of measurements.
    \item Partial state updates that only updates variable estimates if they have changed sufficiently. 
\end{itemize}

In \gsl{iSAM2} the SLAM problem is represented as a factor graph. That means it represents the joint probability of the set of poses and the map as a product of factors, $\prod_i \phi_i$, where each factor is only dependent on a small set of the poses and landmarks. In addition to this, it converts this representation into a Bayes tree, a structure formed by cliques of variables that all depend on each other. The resulting tree is a \gls{DAG} that intuitively shows the dependence of cliques on each other. This allows to only update affected cliques when new measurements arrive.

\gls{iSAM2} assumes the standard form of the measurement model,

\begin{equation}
    \phi_i(\Theta_i) \propto exp(-\frac{1}{2}||h_i(\Theta_i) - z_i||_{\Sigma_i}^2)
\end{equation}

where $\Theta_i$ is the set of variables that $\phi_i$ depends on, and $||e||_{\Sigma}^2$ is the squared Mahalanobis distance\cite{Mahalanobis}. This leads to a nonlinear least squares criterion that can be written in the form 

\begin{equation}
    \argmin_{\Theta}(-log\phi(\Theta)) = \argmin_{\Theta}\frac{1}{2}\sum_i|||h_i(\Theta_i) - z_i||_{\Sigma_i}^2
\end{equation}

Solving this by gradient methods means one solves this by repeatedly linearising and solving the local linear least squares problem

\begin{equation}
    \argmin_{\Delta}||A\Delta - b||^2
\end{equation}

This is done in iSAM2 by factorising the problem through Cholesky factorisation to get the information matrix $A^TA$. This makes the new factorised system

\begin{equation}
    A^TA = A^Tb
\end{equation}

easy to solve by forward and backwards substitution. The aim of iSAM2 is therefore to keep this factorisation unchanged as much as possible, as the linear least squares problem then always is efficient to solve. To do this it keeps track of how much a variable has deviated from its linearisation point, and manages to only relinearise the parts of the cost function that are affected by this variable. This is again done by using the Bayes tree. 

When a new set of measurements, $z_k$, arrive,  all the affected cliques are removed and converted back into a factor graph. Then the new measurements are added as factors and the resulting factor graph is converted back again into a Bayes tree. The rest of the cliques from the previous tree are then added to this new Bayes tree. 

The ordering used to convert the factor graph to a Bayes tree is constrained to get the newest variables to the top of the tree. This is because it is assumed that the newest variables are the most likely to be affected by new measurements. Thus this ordering ensures that the expected total number of variables affected by new measurements is kept to a minimum. This constraint is enforced through CCOLAMD\cite{CCOLAMD}, that also ensures overall good ordering giving sparsity to the factorised information matrix. 

The final piece to the \gls{iSAM2} puzzle is to only update states that are thought to have changed enough. This is done by updating states by going backwards in the Bayes tree. At each clique it determines how much this new update changes the variables in the clique. When the change in variables gets small enough, it stops updating the rest of the variables. This means the estimates coming out of the algorithm are only approximations, but it avoids batch steps and manages to keep the runtime low at all times.

The sum of the above mentioned steps is that \gls{iSAM2} manages to incrementally add information without performing periodic batch operations. This property is crucial in a scenario where SLAM needs to be solved quickly on the fly. Like for example a race car trying to map while driving fast.